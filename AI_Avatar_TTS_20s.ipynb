{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "AI_Avatar_TTS_20s.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# üéØ 20‚ÄëSecond Talking Avatar (Open‚ÄëSource, Colab Ready)\n\nThis notebook will:\n1) Take a **local 30s video** you upload  \n2) **Clone** its voice (using a short reference from the video‚Äôs audio) with **Coqui XTTS v2** (open‚Äësource)  \n3) Generate a **20s text‚Äëto‚Äëspeech** clip in that style  \n4) Create a **realistic talking avatar** (head poses, eye blinks, lip-sync) from a still frame using **SadTalker**  \n5) Output a **final 20s MP4** with audio + avatar\n\n> **Ethics & Rights:** Only use voices & faces you own or have permission to use. Respect local laws and platform policies."
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": "#@title ‚õèÔ∏è Check GPU\nimport torch, platform\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nprint(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\nprint(\"Python:\", platform.python_version())",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": "#@title üì¶ Install dependencies (takes ~2‚Äì5 minutes)\n!sudo apt-get -qq update\n!sudo apt-get -qq install -y ffmpeg git-lfs\n!git lfs install\n\n# Core python deps\n!pip -q install --upgrade pip\n!pip -q install TTS==0.22.0  # Coqui TTS with XTTS v2\n!pip -q install moviepy==1.0.3 librosa==0.10.1 soundfile==0.12.1 numpy==1.26.4\n!pip -q install opencv-python-headless==4.8.0.74\n!pip -q install tqdm==4.66.5\n\n# SadTalker (talking head)\n!git clone -q https://github.com/OpenTalker/SadTalker.git\n%cd SadTalker\n!pip -q install -r requirements.txt\n\n# Download SadTalker pretrained models\n!bash scripts/download_models.sh\n\n# Return to root\n%cd /content",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": "#@title ‚¨ÜÔ∏è Upload your 30‚Äësecond source video\n#@markdown Upload a video with a clear single speaker (frontal if possible). Duration can be >30s; we will trim automatically.\nfrom google.colab import files\nuploaded = files.upload()  # pick your local video\nassert len(uploaded) > 0, \"No file uploaded\"\nsrc_video_path = list(uploaded.keys())[0]\nprint(\"Uploaded:\", src_video_path)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": "#@title üé¨ Extract mid‚Äëframe portrait and a clean 10‚Äësecond voice reference\n#@markdown We pick a mid-frame as the avatar's face; and extract a 10s audio slice for cloning.\nimport subprocess, json, os, cv2\nimport numpy as np\n\nos.makedirs(\"work\", exist_ok=True)\nmid_frame_path = \"work/reference_face.jpg\"\nref_wav_path = \"work/ref_voice_10s.wav\"\ntrimmed_src_path = \"work/source_30s.mp4\"\n\n# 5a) Normalize/trim to 30s (from 00:00)\n!ffmpeg -y -i \"$src_video_path\" -t 30 -r 25 -vf \"scale=768:-2\" -an \"$trimmed_src_path\" -loglevel error\nprint(\"Normalized/trimmed:\", trimmed_src_path)\n\n# 5b) Extract audio (full) then cut a clean 10s window (starting at 5s to avoid intros)\n!ffmpeg -y -i \"$src_video_path\" -vn -ac 1 -ar 16000 \"work/full.wav\" -loglevel error\n!ffmpeg -y -ss 5 -t 10 -i \"work/full.wav\" -ac 1 -ar 16000 \"$ref_wav_path\" -loglevel error\nprint(\"Ref voice:\", ref_wav_path)\n\n# 5c) Grab a middle frame as portrait\n# Use ffprobe to find duration and pick mid timestamp\nimport subprocess, json, math\ndur_json = subprocess.check_output(['ffprobe','-v','error','-show_entries','format=duration','-of','json','-i',src_video_path])\nduration = float(json.loads(dur_json)['format']['duration'])\nmid_ts = max(0.0, duration/2.0)\n\n# Extract a frame image\n!ffmpeg -y -ss {mid_ts} -i \"$src_video_path\" -frames:v 1 \"work/raw_mid.jpg\" -loglevel error\n\n# Face crop (simple center-crop fallback if face not detected)\nimg = cv2.imread(\"work/raw_mid.jpg\")\nh,w = img.shape[:2]\n# center crop to square\nside = min(h,w)\ny0 = (h-side)//2; x0 = (w-side)//2\nsquare = img[y0:y0+side, x0:x0+side]\n# Resize to 512 for SadTalker\nsquare = cv2.resize(square, (512,512), interpolation=cv2.INTER_AREA)\ncv2.imwrite(mid_frame_path, square)\nprint(\"Reference face saved to:\", mid_frame_path)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": "#@title üó£Ô∏è Generate 20‚Äësecond TTS in cloned style (Coqui XTTS v2)\n#@markdown Enter the exact text (‚âà45‚Äì55 words for ~20s at natural pace).\nfrom TTS.api import TTS\nimport soundfile as sf\nimport numpy as np\n\ntts_text = \"Hello! This is a demo of an open source AI avatar. My voice was cloned from the short reference, and I am speaking naturally with expressive prosody and clear articulation. Welcome to your fully local, privacy friendly pipeline powered by open models.\" #@param {type:\"string\"}\nout_tts_wav = \"work/tts_20s.wav\"\n\n# Load multilingual XTTS v2\navail = TTS.list_models()\nmodel_name = \"tts_models/multilingual/multi-dataset/xtts_v2\" if any(\"xtts_v2\" in m for m in avail) else avail[0]\ntts = TTS(model_name)\n\n# Synthesize (language auto; set to 'en' unless you need another)\naudio = tts.tts(text=tts_text, speaker_wav=\"work/ref_voice_10s.wav\", language=\"en\")\nsf.write(out_tts_wav, np.array(audio), 24000)\nprint(\"TTS saved:\", out_tts_wav)\n\n# Optional: force‚Äëtrim/pad to ~20s\n!ffmpeg -y -i \"$out_tts_wav\" -t 20 -ac 1 -ar 24000 \"work/tts_20s_fixed.wav\" -loglevel error\nout_tts_wav = \"work/tts_20s_fixed.wav\"\nprint(\"TTS (20s) saved:\", out_tts_wav)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": "#@title üßë‚Äçüé§ Generate talking avatar with SadTalker\n#@markdown Produces a head-posed, expressive talking head driven by TTS audio.\nimport os, subprocess, shlex, pathlib\nimg = \"work/reference_face.jpg\"\naud = \"work/tts_20s_fixed.wav\"\nout_dir = \"work/sadtalker_out\"\nos.makedirs(out_dir, exist_ok=True)\n\ncmd = f\"\"\"python /content/SadTalker/inference.py   --driven_audio \"{aud}\"   --source_image \"{img}\"   --enhancer gfpgan   --preprocess full   --result_dir \"{out_dir}\"   --still   --expression_scale 1.2   --pose_scale 1.1   --ref_eyeblink \"video\"   --ref_pose \"audio\"   --use_idle   --batch_size 1\n\"\"\"\nprint(\"Running:\", cmd)\nret = subprocess.run(cmd, shell=True, check=False)\nprint(\"Return code:\", ret.returncode)\n\n# Find the generated video\nfrom glob import glob\ncandidates = sorted(glob(os.path.join(out_dir, \"*.mp4\")), key=os.path.getmtime)\nassert candidates, \"No output video produced by SadTalker.\"\ngen_video = candidates[-1]\nprint(\"SadTalker video:\", gen_video)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": "#@title üéöÔ∏è Remux clean audio & export final 20s MP4\nfrom glob import glob\nimport os\n\ngen_video = sorted(glob(\"work/sadtalker_out/*.mp4\"), key=os.path.getmtime)[-1]\nfinal_mp4 = \"AI_Avatar_20s.mp4\"\n\n# Replace audio track with our clean tts_20s_fixed.wav at 24kHz; set fps 25\n!ffmpeg -y -i \"$gen_video\" -i \"work/tts_20s_fixed.wav\" -map 0:v:0 -map 1:a:0 -c:v libx264 -pix_fmt yuv420p -r 25 -c:a aac -b:a 128k -shortest \"$final_mp4\" -loglevel error\nprint(\"Final video:\", final_mp4)",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## ‚úÖ Outputs\n- `AI_Avatar_20s.mp4` ‚Äî final 20s talking avatar video  \n- `work/reference_face.jpg` ‚Äî portrait used by SadTalker  \n- `work/tts_20s_fixed.wav` ‚Äî cloned‚Äëstyle TTS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true
      },
      "source": "#@title ‚¨áÔ∏è Download final video\nfrom google.colab import files\nfiles.download(\"AI_Avatar_20s.mp4\")",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## üõ†Ô∏è Troubleshooting\n\n- **No GPU** ‚Üí Runtime ‚ñ∏ Change runtime type ‚ñ∏ **T4/A100 GPU**.  \n- **SadTalker \"No output video\"** ‚Üí Use a **clear portrait**; try another frame; set `--preprocess crop` and `--still` flags.  \n- **Voice not close** ‚Üí Ensure the 10s reference has **clean speech**; extend `-t 12` in the ref‚Äëaudio cut.  \n- **Laggy lips** ‚Üí Lower `--pose_scale` to `1.0`, try `--expression_scale 1.0`.  \n- **Choppy audio** ‚Üí Re‚Äëencode: `ffmpeg -i in.wav -ac 1 -ar 24000 out.wav`."
    }
  ]
}